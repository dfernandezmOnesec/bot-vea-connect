#!/usr/bin/env python3
"""
Script de ejecuci√≥n para comparaci√≥n end-to-end de bots.

Uso:
    python run_e2e_comparison.py [--config test_data.json] [--output report.html]
"""

import argparse
import sys
from pathlib import Path
from bot_comparison_tool import BotComparisonTool


def main():
    """Funci√≥n principal del script."""
    parser = argparse.ArgumentParser(
        description="Ejecutar comparaci√≥n end-to-end entre bot original y refactorizado"
    )
    parser.add_argument(
        "--config",
        default="tests/e2e/test_data.json",
        help="Ruta al archivo de configuraci√≥n de tests (default: tests/e2e/test_data.json)"
    )
    parser.add_argument(
        "--output",
        default="bot_comparison_report.html",
        help="Ruta del reporte de salida (default: bot_comparison_report.html)"
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="Mostrar informaci√≥n detallada durante la ejecuci√≥n"
    )
    
    args = parser.parse_args()
    
    # Verificar que el archivo de configuraci√≥n existe
    config_path = Path(args.config)
    if not config_path.exists():
        print(f"‚ùå Error: Archivo de configuraci√≥n no encontrado: {config_path}")
        sys.exit(1)
    
    try:
        print("üöÄ Iniciando comparaci√≥n end-to-end de bots...")
        print(f"üìÅ Configuraci√≥n: {config_path}")
        print(f"üìÑ Reporte: {args.output}")
        print("-" * 50)
        
        # Crear herramienta de comparaci√≥n
        tool = BotComparisonTool(str(config_path))
        
        # Ejecutar comparaci√≥n
        report = tool.run_comparison()
        
        # Generar reporte
        report_path = tool.generate_report(report, args.output)
        
        # Mostrar resumen
        print("\n" + "="*60)
        print("üìä RESUMEN DE COMPARACI√ìN END-TO-END")
        print("="*60)
        print(f"üìà Total de tests: {report.total_tests}")
        print(f"‚úÖ Tests exitosos: {report.passed_tests}")
        print(f"‚ùå Regresiones: {report.failed_tests}")
        print(f"üö® Fallos cr√≠ticos: {report.critical_failures}")
        print(f"üìä Similitud promedio: {report.average_similarity:.2%}")
        print(f"‚è±Ô∏è  Tiempo promedio bot original: {report.average_original_time:.1f}ms")
        print(f"‚è±Ô∏è  Tiempo promedio bot refactorizado: {report.average_refactored_time:.1f}ms")
        print(f"üìÑ Reporte generado: {report_path}")
        
        # Mostrar detalles si hay regresiones
        if report.regressions:
            print("\nüö® REGRESIONES DETECTADAS:")
            for regression in report.regressions:
                status = "üö® CR√çTICA" if regression.critical else "‚ö†Ô∏è  NO CR√çTICA"
                print(f"  {status} - Test {regression.test_id}: {regression.regression_reason}")
        
        # C√≥digo de salida
        if report.critical_failures > 0:
            print("\n‚ùå ATENCI√ìN: Se detectaron fallos cr√≠ticos que requieren atenci√≥n inmediata.")
            return 1
        elif report.failed_tests > 0:
            print("\n‚ö†Ô∏è  ADVERTENCIA: Se detectaron regresiones no cr√≠ticas.")
            return 0
        else:
            print("\n‚úÖ Todos los tests pasaron exitosamente.")
            return 0
            
    except Exception as e:
        print(f"‚ùå Error en comparaci√≥n end-to-end: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit(main()) 